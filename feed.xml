<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://aravindskumar.com/feed.xml" rel="self" type="application/atom+xml"/><link href="https://aravindskumar.com/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-01-02T04:03:05+00:00</updated><id>https://aravindskumar.com/feed.xml</id><title type="html">blank</title><subtitle>Aravind Sunil Kumar - AI Systems Engineer at NVIDIA specializing in AI agents for incident analysis, Bayesian models, and LLMs. Contributor to surgical robotics research including da Vinci Research Kit (dVRK), SlicerROS2, and ROS 2 integration. Expert in robotics infrastructure, image-guided surgery, and medical imaging. </subtitle><entry><title type="html">Finally understanding VLA diffusion, I think</title><link href="https://aravindskumar.com/blog/2025/finally-understanding-vla-diffusion-i-think/" rel="alternate" type="text/html" title="Finally understanding VLA diffusion, I think"/><published>2025-12-15T10:05:00+00:00</published><updated>2025-12-15T10:05:00+00:00</updated><id>https://aravindskumar.com/blog/2025/finally-understanding-vla-diffusion-i-think</id><content type="html" xml:base="https://aravindskumar.com/blog/2025/finally-understanding-vla-diffusion-i-think/"><![CDATA[<p>This was something I found uncomfortable while trying to understand Vision Language Action models (VLA): I “knew” all the individual components, CLIP, transformers, diffusion, robot control, but I didn’t actually have a clean mental model of why this worked.</p> <p>Given an image, a language instruction, and the robot’s current state, what exactly is fed into the diffusion model, and what is the model learning to predict? I am not trying to propose a detailed architecture here, but provide a mental-map/intuition of what is going on. (PSA: This describes the <a href="https://diffusion-policy.cs.columbia.edu/" target="_blank" rel="noopener">Diffusion Policy</a>-style architecture. Other VLAs like Other VLAs like <a href="https://robotics-transformer2.github.io/" target="_blank" rel="noopener">RT-2</a> integrate action prediction more tightly into a pretrained Vision Language Model (VLM) backbone.)</p> <p><strong>At a high level, a VLA model is a policy. It takes in vision, language, and state, and outputs actions.</strong></p> <figure> <img src="https://i.ibb.co/35vCJ0wh/VLA.png" alt="High-level overview of a Vision-Language-Action diffusion policy" style="width: 100%; max-width: 1100px; display: block; margin: 0 auto;"/> <figcaption style="text-align: center; font-size: 0.9em; color: #666;"> High-level overview of a Vision–Language–Action diffusion policy. </figcaption> </figure> <h2 id="inputs">Inputs</h2> <ul> <li>State</li> <li>Image (from camera on the robot)</li> <li>Instruction (Given by the human for the task we need to accomplish)</li> </ul> <div style=" background: #f0f7ff; border-left: 4px solid #3b82f6; padding: 1em 1.2em; margin: 1.5em 0; border-radius: 4px; "> <strong>Note for readers new to robotics:</strong> <br/><br/> "robot state" means the information the robot knows about its own body, like joint angles and the gripper's position. This data comes directly from the robot's hardware by just reading sensor values (no perception). Each joint in a robot arm has an encoder that continuously reports its angles, so a 7-DOF arm will get a 7-dimensional vector of joint positions. The gripper (end-effector) adds another dimension or two (how open it is, whether it's grasping something). Some setups also include joint velocities or torques. <br/><br/> The camera setup varies a lot depending on the task. A wrist-mounted camera (eye-in-hand) moves with the end effector and is good for close-up manipulation as you see what the gripper sees. A fixed third-person camera mounted above or beside the workspace gives a broader view of the scene but doesn't move. Many real-world setups use both. <br/><br/> An "action" is how the robot actually moves its body. In most manipulation settings, actions are expressed as small changes to the robot’s joints or end-effector over time, like delta of joint angles, joint velocities, or (though not often) a desired gripper pose. For the robot: <em> how should I move right now given what I see, what I was told to do, and where my body currently is.</em> </div> <p>But these inputs are in different spaces. Images are in pixels, language instructions are text tokens and the robot state is a vector of joint values. Before we can do anything with them, we need to represent them as embeddings.</p> <p>So, to do this:</p> <ul> <li>Vision is passed through an image encoder like a ResNet, ViT, or DINO, producing a high-dimensional vector.</li> <li>Language goes through a text encoder such as CLIP, producing another vector.</li> <li>The robot’s state, such as joint angles and gripper position (pose), is usually passed through a simple linear layer At this stage, each modality is embedded, but they still live in spaces of different dimensionalities and do not yet form a single belief about the world.</li> </ul> <blockquote> <p>Vision answers: “What is where?” Language answers: “What is the task?" State answers: “What can I physically reach or do right now?" For action prediction, we meed to be able to represent "Which object mentioned in the instruction is reachable given the current joint configuration?". To this end, all of them need to live in a shared latent space where they can influence each other through attention.</p> </blockquote> <div class="highlight"><pre><span></span><span class="c1">#Each embedding is projected into a common dimension, say 256.</span>

<span class="n">proj_image</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="mi">2048</span><span class="p">,</span> <span class="mi">256</span><span class="p">)(</span><span class="n">image_embedding</span><span class="p">)</span>
<span class="n">proj_text</span>  <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">256</span><span class="p">)(</span><span class="n">text_embedding</span><span class="p">)</span>
<span class="n">proj_state</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">256</span><span class="p">)(</span><span class="n">state_embedding</span><span class="p">)</span>
</pre></div> <p>&lt;h2 id=representing-the-input-better&gt;Representing the input better&lt;/h2&gt;&lt;p&gt;Now, before it reaches the diffusion steps, we need a way to “represent” these three inputs better. The vectors from these different input sources aren’t contextualized by each other.&lt;/p&gt;</p> <p>To make them interact, the model projects all of these embeddings using learned linear layers. Now that they have the same dimensionality, we are stack them together and pass them through an attention layer (over-simplification).</p> <div class="highlight"><pre><span></span><span class="c1">## This is an over-simplification</span>

<span class="c1"># Stack into a sequence: [image_token, text_token, state_token]</span>
<span class="n">tokens</span> <span class="o">=</span> <span class="n">stack</span><span class="p">([</span><span class="n">proj_image</span><span class="p">,</span> <span class="n">proj_text</span><span class="p">,</span> <span class="n">proj_state</span><span class="p">])</span>  <span class="c1"># shape: (3, 256)</span>

<span class="n">Q</span> <span class="o">=</span> <span class="n">tokens</span> <span class="o">@</span> <span class="n">W_q</span>
<span class="n">K</span> <span class="o">=</span> <span class="n">tokens</span> <span class="o">@</span> <span class="n">W_k</span>
<span class="n">V</span> <span class="o">=</span> <span class="n">tokens</span> <span class="o">@</span> <span class="n">W_v</span>

<span class="n">attention</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">Q</span> <span class="o">@</span> <span class="n">K</span><span class="o">.</span><span class="n">T</span> <span class="o">/</span> <span class="n">sqrt</span><span class="p">(</span><span class="n">d</span><span class="p">))</span>
<span class="n">fused_tokens</span> <span class="o">=</span> <span class="n">attention</span> <span class="o">@</span> <span class="n">V</span>
</pre></div> <p>After this, the image representation becomes aware of the language instruction and the robot’s current configuration. The language representation becomes grounded in what the robot sees. The robot joint state representation becomes contextualized by both the scene and the task. These vectors are no longer purely visual, linguistic, or proprioceptive (fancy word for robot's "self-awareness" of where its parts are in space).</p> <p><strong>How does this reach the diffusion model?</strong> There are two common approaches:</p> <ol> <li><p><strong>Cross-attention</strong>: The fused tokens are passed as-is to the diffusion model, which attends to them via cross-attention layers. The diffusion model queries these context tokens when deciding how to denoise the action.</p> </li> <li><p><strong>Pooled conditioning</strong>: The fused tokens are pooled (e.g., averaged) into a single context vector, which is injected into the diffusion model via concatenation (or something fancier).</p> </li> </ol> <div class="highlight"><pre><span></span><span class="c1"># Option 1: Pass full sequence to diffusion model (cross-attention)</span>
<span class="n">context_tokens</span> <span class="o">=</span> <span class="n">fused_tokens</span>  <span class="c1"># shape: (3, 256)</span>

<span class="c1"># Option 2: Pool into a single vector (simpler conditioning)</span>
<span class="n">context_vector</span> <span class="o">=</span> <span class="n">mean</span><span class="p">(</span><span class="n">fused_tokens</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># shape: (256,)</span>
</pre></div> <p>This is the conditioning signal for the diffusion model.</p> <blockquote> <p>In practice, this fused representation is usually processed by a deep transformer backbone, either explicitly before the diffusion model or implicitly inside the diffusion network via cross-attention. This backbone is where multi-step reasoning and grounding occur: language tokens attend to image regions, state tokens inform reachability, and the model builds a task-aware context. The diffusion model then uses this contextualized representation to iteratively denoise actions.</p> </blockquote> <p>&lt;h2 id=generating-or-learning-actions&gt;Generating (or learning) actions&lt;/h2&gt;&lt;p&gt;<strong><em>In diffusion-based VLAs, the diffusion model operates over actions, not states. The state is known and fixed for a given decision. The action is what we are trying to get.</em></strong> The VLA outputs actions (not states). The robot state is a measurement. It tells the model where the robot is right now. The action is a command. It tells the robot what to do next.&lt;/p&gt;</p> <blockquote> <p>In practice, most diffusion-based VLAs do not model a <em>single</em> action, but a short <strong>action trajectory</strong> or <strong>action chunk</strong> over a fixed horizon. That is, the random variable being diffused is typically <br/> <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>A</mi><mo>&#x0003D;</mo><mo stretchy="false">[</mo><msub><mi>a</mi><mi>t</mi></msub><mo>&#x0002C;</mo><msub><mi>a</mi><mrow><mi>t</mi><mo>&#x0002B;</mo><mn>1</mn></mrow></msub><mo>&#x0002C;</mo><mo>&#x02026;</mo><mo>&#x0002C;</mo><msub><mi>a</mi><mrow><mi>t</mi><mo>&#x0002B;</mo><mi>H</mi><mo>&#x02212;</mo><mn>1</mn></mrow></msub><mo stretchy="false">]</mo></mrow></math> <br/> and at inference time only the first action (or a small prefix) is executed before replanning.</p> </blockquote> <div class="highlight"><pre><span></span><span class="c1"># Robot state: current joint angles (read from encoders)</span>
<span class="n">robot_state</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">joint1_angle</span><span class="p">,</span>   <span class="c1"># e.g., 0.5 radians</span>
    <span class="n">joint2_angle</span><span class="p">,</span>   <span class="c1"># e.g., -0.3 radians</span>
    <span class="n">joint3_angle</span><span class="p">,</span>   <span class="c1"># e.g., 1.2 radians</span>
    <span class="n">joint4_angle</span><span class="p">,</span>   <span class="c1"># e.g., 0.0 radians</span>
    <span class="n">joint5_angle</span><span class="p">,</span>   <span class="c1"># e.g., -0.8 radians</span>
    <span class="n">joint6_angle</span><span class="p">,</span>   <span class="c1"># e.g., 0.4 radians</span>
    <span class="n">joint7_angle</span><span class="p">,</span>   <span class="c1"># e.g., 0.1 radians</span>
    <span class="n">gripper_pos</span>     <span class="c1"># e.g., 1.0 (open)</span>
<span class="p">]</span>

<span class="c1"># Action: velocity or position delta commands</span>
<span class="n">action</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">delta_joint1</span><span class="p">,</span>   <span class="c1"># e.g., +0.1 (move joint 1 forward)</span>
    <span class="n">delta_joint2</span><span class="p">,</span>   <span class="c1"># e.g., -0.05 (move joint 2 back)</span>
    <span class="n">delta_joint3</span><span class="p">,</span>   <span class="c1"># e.g., 0.0 (hold position)</span>
    <span class="n">delta_joint4</span><span class="p">,</span>   <span class="c1"># e.g., 0.0</span>
    <span class="n">delta_joint5</span><span class="p">,</span>   <span class="c1"># e.g., 0.0</span>
    <span class="n">delta_joint6</span><span class="p">,</span>   <span class="c1"># e.g., 0.0</span>
    <span class="n">delta_joint7</span><span class="p">,</span>   <span class="c1"># e.g., 0.0</span>
    <span class="n">gripper_cmd</span>     <span class="c1"># e.g., 1.0 (close gripper)</span>
<span class="p">]</span>
</pre></div> <blockquote> <p>In most implementations, actions are normalized, relative (deltas rather than absolute targets), and often represent velocities or end-effector twists to keep the action distribution smooth and well-behaved under Gaussian noise.</p> </blockquote> <h3 id="training">Training</h3> <p>Diffusion training works by corrupting clean data with noise and teaching a model to reverse that corruption. In this case, the clean data is an expert action from a demonstration. During training, Gaussian noise is added to this action according to a noise schedule (The noise is known. We sample it ourselves). Early in the process, the action is almost clean. Later, it becomes nearly pure noise.</p> <div class="highlight"><pre><span></span><span class="c1"># a_0: clean expert action</span>
<span class="c1"># eps: Gaussian noise</span>
<span class="c1"># alpha_t: noise schedule (ontrols how quickly this transition happen)</span>

<span class="n">a_t</span> <span class="o">=</span> <span class="n">sqrt</span><span class="p">(</span><span class="n">alpha_t</span><span class="p">)</span> <span class="o">*</span> <span class="n">a_0</span> <span class="o">+</span> <span class="n">sqrt</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha_t</span><span class="p">)</span> <span class="o">*</span> <span class="n">eps</span>
</pre></div> <blockquote> <p>“time” appears in three distinct ways here: (1) the diffusion timestep (noise level), (2) the control timestep within the action sequence, and (3) the observation history (past images and states).</p> </blockquote> <p>The training objective is simple: given the noisy action, the timestep, and the context vector, the model is asked to predict the noise that was added.</p> <ul> <li>We are not directly training the model to output actions.</li> <li>It is trained to recognize and remove noise at every noise level.</li> <li>Loss is simple, just mean squared error between the predicted noise and true noise.</li> </ul> <div class="highlight"><pre><span></span><span class="c1"># Assume 1000 steps in the denoising process</span>
<span class="c1"># Clean action (from expert demonstration)</span>
<span class="n">a_0</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]</span>   <span class="c1"># 7 numbers</span>
<span class="c1"># Random noise (sampled from standard normal)</span>
<span class="n">ε</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3</span><span class="p">,</span><span class="mf">0.2</span><span class="p">]</span>    <span class="c1"># 7 numbers</span>

<span class="c1"># At t=500 (midway through diffusion), α_t ≈ 0.5</span>
<span class="n">a_t</span> <span class="o">=</span> <span class="n">sqrt</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span> <span class="o">*</span> <span class="n">a_0</span> <span class="o">+</span> <span class="n">sqrt</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span> <span class="o">*</span> <span class="n">ε</span>
<span class="c1">#    = 0.707 * [0.1, -0.05, ...] + 0.707 * [0.3, -0.2, ...]</span>
<span class="c1">#    = [0.28, -0.18, 0.35, -0.07, 0.28, -0.21, 0.85]</span>

<span class="c1"># At t=1000 (end): a_t ≈ ε (pure noise)</span>
<span class="c1"># At t=0 (start): a_t = a_0 (clean action)</span>
</pre></div> <p><strong>By learning to do this well across many timesteps and many contexts, the model implicitly learns the conditional distribution of actions given vision, language, and state.</strong></p> <div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span>
    <span class="n">image</span><span class="p">,</span> <span class="n">instruction</span><span class="p">,</span> <span class="n">robot_state</span><span class="p">,</span> <span class="n">a_0</span> <span class="o">=</span> <span class="n">batch</span>

    <span class="c1"># Encode and fuse context</span>
    <span class="n">context</span> <span class="o">=</span> <span class="n">encode_and_fuse</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">instruction</span><span class="p">,</span> <span class="n">robot_state</span><span class="p">)</span>

    <span class="c1"># Sample noise</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn_like</span><span class="p">(</span><span class="n">a_0</span><span class="p">)</span>

    <span class="c1"># Sample diffusion timestep</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">T</span><span class="p">)</span>

    <span class="c1"># Create noisy action</span>
    <span class="n">a_t</span> <span class="o">=</span> <span class="n">sqrt</span><span class="p">(</span><span class="n">alpha_t</span><span class="p">)</span> <span class="o">*</span> <span class="n">a_0</span> <span class="o">+</span> <span class="n">sqrt</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha_t</span><span class="p">)</span> <span class="o">*</span> <span class="n">eps</span>

    <span class="c1"># Predict the noise</span>
    <span class="n">eps_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">a_t</span><span class="p">,</span> <span class="n">context</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>

    <span class="c1"># Train to recover the noise</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">MSE</span><span class="p">(</span><span class="n">eps_pred</span><span class="p">,</span> <span class="n">eps</span><span class="p">)</span>
</pre></div> <h3 id="inference">Inference</h3> <p>At inference time, the setup is similar (but inverted and there is no expert action). We use the model as a learned denoiser that is guided by context.</p> <ul> <li>We start with pure noise as the action vector.</li> <li>encode the current observation into a context representation, like during training.</li> <li>Iteratively run the diffusion model to predict noise and remove it step by step.</li> <li>After enough denoising steps, what remains is a clean action (or action trajectory) that is consistent with the observation and instruction (hopefully!).</li> <li>That action is then sent to the robot.</li> </ul> <p>We have a new state now. Rinse and repeat (like in the image up top)</p> <div class="highlight"><pre><span></span>a = torch.randn(action_dim)

for t in reversed(range(T)):
    eps_pred = model(a, context, t)
    a = denoise_step(a, eps_pred, t)
</pre></div> <div style=" border-left: 4px solid #f0f7ff; padding: 0.75em 1em; margin: 1.5em 0; background: #eef5ff; "> <strong>Note.</strong> <p> The core problem with naive behavior cloning (supervised learning from demonstrations) is multimodality. When different demonstrators push an object left versus right to achieve the same goal, a policy trained with an MSE loss learns the average behavior, which might correspond to pushing straight into the object. This averaging problem is catastrophic for robot control. </p> <p> Diffusion policies avoid this by modeling an implicit <strong>distribution over action trajectories</strong> rather than regressing a single action. At inference time, the model samples and commits to one valid mode instead of averaging across modes. This is analogous to image diffusion models, which generate sharp, distinct samples rather than blurry averages. </p> </div> <p>Specific implementations vary on whether they use transformers or other sequence models, DDPM or flow matching, but the core idea remains the same.</p> <p><strong>tldr</strong>: Raw inputs come in as an image, a language instruction, and the robot’s current joint configuration. Each modality is encoded, projected into a shared space, and fused. The resulting context representation conditions a diffusion model that operates over actions (typically short action trajectories rather than single control commands). During training, the model learns to predict known noise added to expert actions. During inference, it starts from noise and iteratively denoises to produce a valid action.</p> <h2 id="references">References</h2> <p><strong>VLA / Robotics:</strong></p> <ol> <li><p><strong>Diffusion Policy</strong>: Chi et al., "Diffusion Policy: Visuomotor Policy Learning via Action Diffusion," RSS 2023. <a href="https://diffusion-policy.cs.columbia.edu/">diffusion-policy.cs.columbia.edu</a> | <a href="https://arxiv.org/abs/2303.04137">arXiv:2303.04137</a></p> </li> <li><p><strong>RT-2</strong>: Brohan et al., "RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control," 2023. <a href="https://robotics-transformer2.github.io/">robotics-transformer2.github.io</a> | <a href="https://arxiv.org/abs/2307.15818">arXiv:2307.15818</a></p> </li> </ol> <p><strong>Diffusion Foundations:</strong></p> <ol start="3"> <li><strong>DDPM</strong>: Ho et al., "Denoising Diffusion Probabilistic Models," NeurIPS 2020. <a href="https://arxiv.org/abs/2006.11239">arXiv:2006.11239</a></li> </ol> <p><strong>Tutorials:</strong></p> <ol start="5"> <li><strong>MIT IAP 2025 Course</strong>: Holderrieth &amp; Erives, "Introduction to Flow Matching and Diffusion Models." <a href="https://diffusion.csail.mit.edu/">diffusion.csail.mit.edu</a></li> </ol>]]></content><author><name></name></author></entry></feed>